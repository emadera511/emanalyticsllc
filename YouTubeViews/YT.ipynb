{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/emanalytics/spark-3.3.1-bin-hadoop3')\n",
    "\n",
    "import pyspark \n",
    "import os \n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "def create_spark_session(): \n",
    "    spark = SparkSession.builder.appName('sparkify').getOrCreate()\n",
    "    return spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/30 10:05:57 WARN Utils: Your hostname, emanalytics-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/12/30 10:05:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/30 10:06:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=create_spark_session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- etag: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- etag: string (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- kind: string (nullable = true)\n",
      " |    |    |-- snippet: struct (nullable = true)\n",
      " |    |    |    |-- assignable: boolean (nullable = true)\n",
      " |    |    |    |-- channelId: string (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |-- kind: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('data/CA_category_id.json', multiLine=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[items: array<struct<etag:string,id:string,kind:string,snippet:struct<assignable:boolean,channelId:string,title:string>>>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn('data', f.explode('items')).select('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.withColumn('id', f.col(\"data.id\")) \\\n",
    "    .withColumn('title', f.col(\"data.snippet.title\")).select('id', 'title').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/CA_category_id.json\n",
      "CA\n",
      "data/JP_category_id.json\n",
      "JP\n",
      "data/MX_category_id.json\n",
      "MX\n",
      "data/DE_category_id.json\n",
      "DE\n",
      "data/US_category_id.json\n",
      "US\n",
      "data/RU_category_id.json\n",
      "RU\n",
      "data/GB_category_id.json\n",
      "GB\n",
      "data/IN_category_id.json\n",
      "IN\n",
      "data/KR_category_id.json\n",
      "KR\n",
      "data/FR_category_id.json\n",
      "FR\n"
     ]
    }
   ],
   "source": [
    "dir = glob.glob('data/*.json')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True), \n",
    "    StructField('title', StringType(), True), \n",
    "    StructField('region', StringType(), True)\n",
    "])\n",
    "\n",
    "final = spark.createDataFrame([], schema)\n",
    "for fn in dir:\n",
    "    print(fn)\n",
    "    a = fn.split('/')\n",
    "    b = a[1].split('_')\n",
    "    print(b[0])\n",
    "    df = spark.read.format('org.apache.spark.sql.json').option('multiline', 'true').load(fn)\n",
    "    df1 = df.withColumn('data', f.explode('items')).select('data').dropDuplicates()\n",
    "    df2 = df1.withColumn('id', f.col(\"data.id\")) \\\n",
    "                .withColumn('title', f.col(\"data.snippet.title\")) \\\n",
    "                .withColumn('region', f.lit(b[0])).select('id', 'title', 'region').dropDuplicates()\n",
    "    final = final.union(df2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 489:=======================================>               (10 + 4) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|region|count|\n",
      "+------+-----+\n",
      "|    CA|   31|\n",
      "|    JP|   31|\n",
      "|    MX|   31|\n",
      "|    DE|   31|\n",
      "|    US|   32|\n",
      "|    RU|   31|\n",
      "|    GB|   31|\n",
      "|    IN|   31|\n",
      "|    KR|   31|\n",
      "|    FR|   31|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#final.filter(final['region']=='CA').sort('id').show()\n",
    "final.groupBy('region').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn('data', f.explode('items')).select('data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+\n",
      "| id|               title| filename|\n",
      "+---+--------------------+---------+\n",
      "|  1|    Film & Animation|Something|\n",
      "|  2|    Autos & Vehicles|Something|\n",
      "| 10|               Music|Something|\n",
      "| 15|      Pets & Animals|Something|\n",
      "| 17|              Sports|Something|\n",
      "| 18|        Short Movies|Something|\n",
      "| 19|     Travel & Events|Something|\n",
      "| 20|              Gaming|Something|\n",
      "| 21|       Videoblogging|Something|\n",
      "| 22|      People & Blogs|Something|\n",
      "| 23|              Comedy|Something|\n",
      "| 24|       Entertainment|Something|\n",
      "| 25|     News & Politics|Something|\n",
      "| 26|       Howto & Style|Something|\n",
      "| 27|           Education|Something|\n",
      "| 28|Science & Technology|Something|\n",
      "| 30|              Movies|Something|\n",
      "| 31|     Anime/Animation|Something|\n",
      "| 32|    Action/Adventure|Something|\n",
      "| 33|            Classics|Something|\n",
      "+---+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('id', f.col(\"data.id\")) \\\n",
    "    .withColumn('title', f.col(\"data.snippet.title\")) \\\n",
    "    .withColumn('filename', f.lit('Something')).select('id', 'title', 'filename')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/CA_category_id.json\n",
      "CA\n",
      "data/JP_category_id.json\n",
      "JP\n",
      "data/MX_category_id.json\n",
      "MX\n",
      "data/DE_category_id.json\n",
      "DE\n",
      "data/US_category_id.json\n",
      "US\n",
      "data/RU_category_id.json\n",
      "RU\n",
      "data/GB_category_id.json\n",
      "GB\n",
      "data/IN_category_id.json\n",
      "IN\n",
      "data/KR_category_id.json\n",
      "KR\n",
      "data/FR_category_id.json\n",
      "FR\n"
     ]
    }
   ],
   "source": [
    "dir = glob.glob('data/*.json')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True), \n",
    "    StructField('title', StringType(), True), \n",
    "    StructField('region', StringType(), True)\n",
    "])\n",
    "\n",
    "final = spark.createDataFrame([], schema)\n",
    "for fn in dir:\n",
    "    print(fn)\n",
    "    #print(file)\n",
    "    a = fn.split('/')\n",
    "    b = a[1].split('_')\n",
    "    print(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 454:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([], StructType([]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dir = glob.glob('data/landing/*.csv')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('video_id', IntegerType(), True), \n",
    "    StructField('trending_date', DateType(), True), \n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('channel_title', IntegerType(), True), \n",
    "    StructField('category_id', IntegerType(), True), \n",
    "    StructField('tags', IntegerType(), True), \n",
    "    StructField('views', IntegerType(), True), \n",
    "    StructField('likes', IntegerType(), True),\n",
    "    StructField('dislikes', IntegerType(), True), \n",
    "    StructField('comment_count', IntegerType(), True), \n",
    "    StructField('region', StringType(), True)\n",
    " \n",
    "])\n",
    "\n",
    "ytfinal = spark.createDataFrame([], schema)\n",
    "\n",
    "for file in dir: \n",
    "    #print(file)\n",
    "    a = file.split('/')\n",
    "    b = a[2]\n",
    "    print(b[0:2])\n",
    "\n",
    "    df = spark.read.csv(file, header=True, inferSchema=True)\n",
    "\n",
    "    df2 = df.select('video_id', \\\n",
    "    'trending_date',  \\\n",
    "    'title', \\\n",
    "    'channel_title', \\\n",
    "    'category_id', \\\n",
    "    'tags', \\\n",
    "    'views', \\\n",
    "    'likes', \\\n",
    "    'dislikes', \\\n",
    "    'comment_count')\n",
    "\n",
    "    df3 = df2.withColumn('region', f.lit(b[0:2]))\n",
    "\n",
    "\n",
    "    ytfinal = ytfinal.union(df3)\n",
    "    #ytfinal = ytfinal.withColumn('region', f.lit(b[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 632:===================================================>   (41 + 3) / 44]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|region|count|\n",
      "+------+-----+\n",
      "|    GB|43295|\n",
      "|    IN|38528|\n",
      "|    US|48137|\n",
      "|    JP|21445|\n",
      "|    FR|46138|\n",
      "|    CA|45560|\n",
      "|    MX|43819|\n",
      "|    KR|36730|\n",
      "|    RU|46260|\n",
      "|    DE|46957|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ytfinal.groupBy('region').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 561:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------+-------+--------+-------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|                tags|   views|  likes|dislikes|comment_count|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------+-------+--------+-------------+\n",
      "|Jw1Y-zhQURU|     17.14.11|John Lewis Christ...|          John Lewis|         26|\"christmas\"|\"john...| 7224515|  55681|   10247|         9479|\n",
      "|3s1rvMFUweQ|     17.14.11|Taylor Swift: …Re...| Saturday Night Live|         24|\"SNL\"|\"Saturday N...| 1053632|  25561|    2294|         2757|\n",
      "|n1WpP7iowLc|     17.14.11|Eminem - Walk On ...|          EminemVEVO|         10|\"Eminem\"|\"Walk\"|\"...|17158579| 787420|   43420|       125882|\n",
      "|PUTEiSjKwJU|     17.14.11|Goals from Salfor...|Salford City Foot...|         17|\"Salford City FC\"...|   27833|    193|      12|           37|\n",
      "|rHwDegptbI4|     17.14.11|Dashcam captures ...|    Cute Girl Videos|         25|              [none]|    9815|     30|       2|           30|\n",
      "|AumaWl0TNBo|     17.14.11|How My Relationsh...|  PointlessBlogVlogs|         24|\"pointlessblog\"|\"...| 1182775|  52708|    1431|         2333|\n",
      "|2Vv-BfVoq4g|     17.14.11|Ed Sheeran - Perf...|          Ed Sheeran|         10|\"edsheeran\"|\"ed s...|33523622|1634124|   21082|        85067|\n",
      "|-N5eucPMTTc|     17.14.11|CHRISTMAS HAS GON...|          MoreZoella|         22|\"zoe sugg\"|\"zoell...| 1164201|  57309|     749|          624|\n",
      "|LMCuKltaY3M|     17.14.11|Elbow - Golden Sl...|           ElbowVEVO|         10|\"Elbow\"|\"Golden\"|...|  154494|   2163|     147|          211|\n",
      "|9t9u_yPEidY|     17.14.11|Jennifer Lopez - ...|   JenniferLopezVEVO|         10|\"Jennifer Lopez f...| 9548677| 190084|   15015|        11473|\n",
      "|ONQ-fAp5X64|     17.14.11|CAN BABIES DO GYM...|         Nile Wilson|         17|\"nile wilson\"|\"ni...|  306724|   9591|     266|          448|\n",
      "|Dlwf5u2BLAg|     17.14.11|TV Reporter Inter...|      Inside Edition|         25|\"reporter\"|\"robbe...|  254314|   1746|     125|          298|\n",
      "|NVCqz2BDfcQ|     17.14.11|Gucci Mane  - Cur...|   OfficialGucciMane|         10|\"Gucci Mane\"|\"The...|  946859|  37986|     816|         2006|\n",
      "|MyEqfBkzESU|     17.14.11|EXTREME GOLDEN TA...|        John Maclean|         26|\"Extreme Tan\"|\"Ta...|  300617|   8133|    2693|         2911|\n",
      "|b4vTZx_AtHk|     17.14.11|How To Dry a Shir...|          HowToBasic|         26|\"how to dry a shi...| 2063667|  98804|   11682|        11443|\n",
      "|BsfhHKx6ajA|     17.14.11|Attracting An Alp...|Good Mythical Mor...|         24|\"rhett and link\"|...|  728547|  12553|    2146|         1143|\n",
      "|DIU3xPdhCBI|     17.14.11|Sam Smith surpris...|           BBC Music|         10|\"bbc\"|\"music\"|\"bb...|  893462|  22326|     833|          943|\n",
      "|pz95u3UVpaM|     17.14.11|Camila Cabello - ...|   CamilaCabelloVEVO|         10|\"camila cabello\"|...| 5476738| 286269|    4083|        12254|\n",
      "|jp9hK-jY6yY|     17.14.11|When Someone Has ...|      IISuperwomanII|         23|\"iisuperwomanii\"|...| 3371669| 202676|    3394|        20086|\n",
      "|gEHCXl4J9Qo|     17.14.11|Taylor Swift - “N...|ABC Television Ne...|         24|\"American Broadca...| 2237404|  72593|    1830|         3501|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------+-------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "['video_id', 'trending_date', 'title', 'channel_title', 'category_id', 'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count', 'thumbnail_link', 'comments_disabled', 'ratings_disabled', 'video_error_or_removed', 'description']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ytfinal.show()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('video_id', IntegerType(), True), \n",
    "    StructField('trending_date', DateType(), True), \n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('channel_title', IntegerType(), True), \n",
    "    StructField('category_id', IntegerType(), True), \n",
    "    StructField('tags', IntegerType(), True), \n",
    "    StructField('views', IntegerType(), True), \n",
    "    StructField('likes', IntegerType(), True),\n",
    "    StructField('dislikes', IntegerType(), True), \n",
    "    StructField('comment_count', IntegerType(), True), \n",
    " \n",
    "])\n",
    "\n",
    "print(hds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------+------+--------+-------------+\n",
      "|   video_id|trending_date|               title|       channel_title|category_id|                tags|   views| likes|dislikes|comment_count|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------+------+--------+-------------+\n",
      "|kzwfHumJyYc|     17.14.11|Sharry Mann: Cute...|     Lokdhun Punjabi|          1|\"sharry mann|\"\"sh...| 1096327| 33966|     798|          882|\n",
      "|zUZ1z7FwLc8|     17.14.11|पीरियड्स के समय, ...|             HJ NEWS|         25|\"पीरियड्स के समय|...|  590101|   735|     904|            0|\n",
      "|10L1hZ9qa58|     17.14.11|Stylish Star Allu...|                TFPC|         24|\"Stylish Star All...|  473988|  2011|     243|          149|\n",
      "|N1vE8iiEg64|     17.14.11|Eruma Saani | Tam...|         Eruma Saani|         23|\"Eruma Saani|\"\"Ta...| 1242680| 70353|    1624|         2684|\n",
      "|kJzGH0PVQHQ|     17.14.11|why Samantha beca...|          Filmylooks|         24|\"Filmylooks|\"\"lat...|  464015|   492|     293|           66|\n",
      "|il_pSa5l98w|     17.14.11|MCA (Middle Class...|            Dil Raju|         24|\"Nenu Local|\"\"Nen...| 6106669| 98612|    4185|         4763|\n",
      "|7MxiQ4v0EnE|     17.14.11|Daang ( Full Vide...|       Speed Records|         10|\"punjabi songs|\"\"...| 5718766|127477|    7134|         8063|\n",
      "|c64I9HNpiOY|     17.14.11|Padmavati : Ek Di...|            T-Series|         10|\"Ek Dil Ek Jaan V...|10588371|132738|    8812|        10847|\n",
      "|KObFEYCaRx8|     17.14.11|Chiranjeevi in Na...|    Top Telugu Media|         24|\"Chiranjeevi in N...|  118223|   520|      53|           23|\n",
      "|g8QsfJhFpjY|     17.14.11|New bike vs Old b...|           Jump Cuts|         24|\"Jump cuts|\"\"Jump...|  969030| 59798|    1545|         2404|\n",
      "|4du1KXBl4YU|     17.14.11|Mehjabi Reveals H...|         TellyMasala|         24|\"Hina khan|\"\"Hina...|  632747|  4330|    2183|         2869|\n",
      "|zOOmEDwPJX0|     17.14.11|Jannat (Full Song...|    White Hill Music|         10|\"Aatish new punja...| 2348107| 32834|     710|         1743|\n",
      "|Prb_osSVE0M|     17.14.11|Renu Desai Gives ...|          ABN Telugu|         25|\"Resnu desai|\"\"ac...|  156085|   716|      53|            0|\n",
      "|KHxdrECEsD0|     17.14.11|Peehu Srivastav P...|The Voice India Kids|         24|\"the voice of ind...|  472413|  2611|     250|          174|\n",
      "|so0ccfs-psI|     17.14.11|Rowi Na | Nadha V...|          VS RECORDS|         10|\"rowi na|\"\"nadha ...|  836006| 24460|     180|          594|\n",
      "|7gShaSuK_PY|     17.14.11|ஜெயலலிதாவின் உயில...|            Next Gen|         22|\"nextgen|\"\"sasika...|   89531|   238|      59|           18|\n",
      "|ep3SLMuRbvg|     17.14.11|TYPES OF STUDENTS...|        Elvish yadav|         23|\"TYPES OF STUDENT...|  344545| 25717|     417|         2870|\n",
      "|ePO5M5DE01I|     17.14.11|Tiger Zinda Hai |...|                 YRF|          1|\"Tiger Zinda Hai ...|35885754|829362|   61195|       101117|\n",
      "|CLl1RbxDRAs|     17.14.11|Meri Setting Karw...|   TroubleSeekerTeam|         23|\"Prank Call|\"\"Fun...|  209599| 14070|     448|         1105|\n",
      "|1ZAPwfrtAFY|     17.14.11|The Trump Preside...|     LastWeekTonight|         24|\"last week tonigh...| 2418783| 97187|    6146|        12703|\n",
      "+-----------+-------------+--------------------+--------------------+-----------+--------------------+--------+------+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('video_id', \\\n",
    "    'trending_date',  \\\n",
    "    'title', \\\n",
    "    'channel_title', \\\n",
    "    'category_id', \\\n",
    "    'tags', \\\n",
    "    'views', \\\n",
    "    'likes', \\\n",
    "    'dislikes', \\\n",
    "    'comment_count').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
